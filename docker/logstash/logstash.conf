# Logstash Configuration for E-commerce Microservices
# Processes logs from all services and sends to Elasticsearch

# INPUT: Receive logs from services
input {
  # TCP input for application logs (Logback)
  tcp {
    port => 5000
    codec => json_lines
    tags => ["application"]
  }
  
  # Beats input for file-based logs
  beats {
    port => 5044
    tags => ["beats"]
  }
  
  # HTTP input for ad-hoc log submission
  http {
    port => 8080
    codec => json
    tags => ["http"]
  }
}

# FILTER: Process and enrich logs
filter {
  # Parse JSON logs
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      skip_on_invalid_json => true
    }
  }
  
  # Add timestamp if not present
  if ![timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }
  
  # Extract service name from logger or use default
  if [logger_name] {
    grok {
      match => { "logger_name" => "com\.ecommerce\.%{WORD:service_name}" }
    }
  }
  
  # Classify log levels
  if [level] {
    mutate {
      lowercase => [ "level" ]
    }
  }
  
  # Add environment tag
  mutate {
    add_field => { "environment" => "development" }
  }
  
  # Extract trace ID for distributed tracing correlation
  if [traceId] {
    mutate {
      rename => { "traceId" => "trace_id" }
    }
  }
  
  # Geolocation for IP addresses (if present)
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
}

# OUTPUT: Send processed logs to Elasticsearch
output {
  # All logs go to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    # Index pattern: logs-servicename-YYYY.MM.DD
    index => "logs-%{[service_name]:unknown}-%{+YYYY.MM.dd}"
    # Document type
    document_type => "log"
  }
  
  # Also output to console for debugging (optional)
  stdout {
    codec => rubydebug
  }
}

